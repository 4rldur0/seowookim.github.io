{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UWndD4-vaI4L"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer"
      ],
      "metadata": {
        "id": "HGd7f9d0fcBj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "XnKpUngpf-7S"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "jropRtJ3gP8L"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)\n",
        "print(sns.__version__)\n",
        "print(np.__version__)\n",
        "print(pd.__version__)\n",
        "print(sklearn.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2QlPuaGaTdc",
        "outputId": "be787c12-93fe-46d7-fe1a-610e14d7cd92"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n",
            "0.13.1\n",
            "1.25.2\n",
            "2.0.3\n",
            "1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1. 빈도수 단어 - 1000, 3000, 5000 적용해보기**"
      ],
      "metadata": {
        "id": "OYUhObILbCYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import reuters\n",
        "\n",
        "freq = [1000,3000,5000]"
      ],
      "metadata": {
        "id": "opw0KzxFaM5R"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import reuters\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "freq = [1000, 3000, 5000]\n",
        "\n",
        "for i in freq:\n",
        "    (x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=i, test_split=0.2)\n",
        "\n",
        "    # 단어 인덱스를 필터링하는 작업이 필요 없음. 이미 num_words에 따라 필터링됨.\n",
        "\n",
        "    print(f'훈련 샘플의 수_{i}: {len(x_train)}')\n",
        "    print(f'테스트 샘플의 수_{i}: {len(x_test)}')\n",
        "    print(f'훈련용 뉴스의 최대 길이_{i}: {max(len(l) for l in x_train)}')\n",
        "    print(f'훈련용 뉴스의 평균 길이_{i}: {sum(map(len, x_train)) / len(x_train)}')\n",
        "\n",
        "    unique_elements, counts_elements = np.unique(y_train, return_counts=True)\n",
        "    print(f\"각 클래스 빈도수_{i}:\")\n",
        "    print(np.asarray((unique_elements, counts_elements)))\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9kFMuZBafTV",
        "outputId": "8a82a524-6ef5-49b6-94b3-a7e87dbaf998"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 샘플의 수_1000: 8982\n",
            "테스트 샘플의 수_1000: 2246\n",
            "훈련용 뉴스의 최대 길이_1000: 2376\n",
            "훈련용 뉴스의 평균 길이_1000: 145.5398574927633\n",
            "각 클래스 빈도수_1000:\n",
            "[[   0    1    2    3    4    5    6    7    8    9   10   11   12   13\n",
            "    14   15   16   17   18   19   20   21   22   23   24   25   26   27\n",
            "    28   29   30   31   32   33   34   35   36   37   38   39   40   41\n",
            "    42   43   44   45]\n",
            " [  55  432   74 3159 1949   17   48   16  139  101  124  390   49  172\n",
            "    26   20  444   39   66  549  269  100   15   41   62   92   24   15\n",
            "    48   19   45   39   32   11   50   10   49   19   19   24   36   30\n",
            "    13   21   12   18]]\n",
            "훈련 샘플의 수_3000: 8982\n",
            "테스트 샘플의 수_3000: 2246\n",
            "훈련용 뉴스의 최대 길이_3000: 2376\n",
            "훈련용 뉴스의 평균 길이_3000: 145.5398574927633\n",
            "각 클래스 빈도수_3000:\n",
            "[[   0    1    2    3    4    5    6    7    8    9   10   11   12   13\n",
            "    14   15   16   17   18   19   20   21   22   23   24   25   26   27\n",
            "    28   29   30   31   32   33   34   35   36   37   38   39   40   41\n",
            "    42   43   44   45]\n",
            " [  55  432   74 3159 1949   17   48   16  139  101  124  390   49  172\n",
            "    26   20  444   39   66  549  269  100   15   41   62   92   24   15\n",
            "    48   19   45   39   32   11   50   10   49   19   19   24   36   30\n",
            "    13   21   12   18]]\n",
            "훈련 샘플의 수_5000: 8982\n",
            "테스트 샘플의 수_5000: 2246\n",
            "훈련용 뉴스의 최대 길이_5000: 2376\n",
            "훈련용 뉴스의 평균 길이_5000: 145.5398574927633\n",
            "각 클래스 빈도수_5000:\n",
            "[[   0    1    2    3    4    5    6    7    8    9   10   11   12   13\n",
            "    14   15   16   17   18   19   20   21   22   23   24   25   26   27\n",
            "    28   29   30   31   32   33   34   35   36   37   38   39   40   41\n",
            "    42   43   44   45]\n",
            " [  55  432   74 3159 1949   17   48   16  139  101  124  390   49  172\n",
            "    26   20  444   39   66  549  269  100   15   41   62   92   24   15\n",
            "    48   19   45   39   32   11   50   10   49   19   19   24   36   30\n",
            "    13   21   12   18]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> 이 뉴스 데이터는 3번, 4번 클래스가 대부분을 차지하고 있습니다. 그 뒤로는 19번,16번, 1번, 11번 등이 높은 분포를 가지고 있네요."
      ],
      "metadata": {
        "id": "8N4C1xrjec93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = reuters.get_word_index()\n"
      ],
      "metadata": {
        "id": "MSkAbatZcvlu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import reuters\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Reuters 데이터셋 로드\n",
        "word_index = reuters.get_word_index()\n",
        "freq = [1000, 3000, 5000]\n",
        "\n",
        "# word_index의 키-값을 뒤집어 역매핑하기 쉽게 만듭니다.\n",
        "reverse_word_index = {value: key for key, value in word_index.items()}\n",
        "\n",
        "for i in freq:\n",
        "    (x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=i, test_split=0.2)\n",
        "\n",
        "    decoded_train = []\n",
        "    decoded_test = []\n",
        "\n",
        "    # 훈련 데이터 디코딩\n",
        "    for seq in x_train:\n",
        "        decoded_seq = ' '.join([reverse_word_index.get(index - 3, '?') for index in seq])\n",
        "        if decoded_seq.strip():  # 빈 문장은 제외\n",
        "            decoded_train.append(decoded_seq)\n",
        "\n",
        "    # 테스트 데이터 디코딩\n",
        "    for seq in x_test:\n",
        "        decoded_seq = ' '.join([reverse_word_index.get(index - 3, '?') for index in seq])\n",
        "        if decoded_seq.strip():  # 빈 문장은 제외\n",
        "            decoded_test.append(decoded_seq)\n",
        "\n",
        "    if not decoded_train or not decoded_test:\n",
        "        print(f\"Warning: No valid documents found for num_words={i}\")\n",
        "        continue\n",
        "\n",
        "    x_train_i = decoded_train\n",
        "    x_test_i = decoded_test\n",
        "\n",
        "    # DTM 생성하기\n",
        "    dtmvector = CountVectorizer(stop_words=None)\n",
        "    x_train_dtm_i = dtmvector.fit_transform(x_train_i)\n",
        "    x_test_dtm_i = dtmvector.transform(x_test_i)\n",
        "\n",
        "    # TF-IDF 생성하기\n",
        "    tfidf_transformer = TfidfTransformer()\n",
        "    x_train_tfidf_i = tfidf_transformer.fit_transform(x_train_dtm_i)\n",
        "    x_test_tfidf_i = tfidf_transformer.transform(x_test_dtm_i)\n",
        "\n",
        "    # 나이브 베이즈 분류기\n",
        "    model_NB = MultinomialNB().fit(x_train_tfidf_i, y_train)\n",
        "\n",
        "    # 예측하기\n",
        "    predicted_i = model_NB.predict(x_test_tfidf_i)\n",
        "    print(f\"정확도_나이브_{i}: {accuracy_score(y_test, predicted_i)}\")\n",
        "    print(classification_report(y_test, predicted_i, zero_division=0))\n",
        "\n",
        "    # 랜덤포레스트\n",
        "    model_RF = RandomForestClassifier(random_state=0).fit(x_train_tfidf_i, y_train)\n",
        "    predicted_i = model_RF.predict(x_test_tfidf_i)\n",
        "    print(f\"정확도_랜덤포레스트_{i}: {accuracy_score(y_test, predicted_i)}\")\n",
        "    print(classification_report(y_test, predicted_i, zero_division=0))\n",
        "\n",
        "    # 그래디언트 부스팅 트리\n",
        "    model_GB = GradientBoostingClassifier(random_state=0).fit(x_train_tfidf_i, y_train)\n",
        "    predicted_i = model_GB.predict(x_test_tfidf_i)\n",
        "    print(f\"정확도_그래디언트부스팅트리_{i}: {accuracy_score(y_test, predicted_i)}\")\n",
        "    print(classification_report(y_test, predicted_i, zero_division=0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcolDP15jKkH",
        "outputId": "484a9659-e2d6-4356-d769-d3f38df10ae0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정확도_나이브_1000: 0.7079252003561888\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.33      0.47        12\n",
            "           1       0.45      0.80      0.58       105\n",
            "           2       0.00      0.00      0.00        20\n",
            "           3       0.93      0.87      0.90       813\n",
            "           4       0.67      0.93      0.78       474\n",
            "           5       0.00      0.00      0.00         5\n",
            "           6       1.00      0.07      0.13        14\n",
            "           7       0.00      0.00      0.00         3\n",
            "           8       0.68      0.39      0.50        38\n",
            "           9       0.92      0.48      0.63        25\n",
            "          10       0.88      0.23      0.37        30\n",
            "          11       0.43      0.83      0.57        83\n",
            "          12       0.00      0.00      0.00        13\n",
            "          13       0.61      0.38      0.47        37\n",
            "          14       0.00      0.00      0.00         2\n",
            "          15       0.00      0.00      0.00         9\n",
            "          16       0.53      0.75      0.62        99\n",
            "          17       0.00      0.00      0.00        12\n",
            "          18       0.50      0.50      0.50        20\n",
            "          19       0.55      0.78      0.65       133\n",
            "          20       0.76      0.37      0.50        70\n",
            "          21       0.83      0.56      0.67        27\n",
            "          22       0.00      0.00      0.00         7\n",
            "          23       0.00      0.00      0.00        12\n",
            "          24       0.00      0.00      0.00        19\n",
            "          25       0.88      0.23      0.36        31\n",
            "          26       0.00      0.00      0.00         8\n",
            "          27       0.00      0.00      0.00         4\n",
            "          28       0.00      0.00      0.00        10\n",
            "          29       0.00      0.00      0.00         4\n",
            "          30       0.00      0.00      0.00        12\n",
            "          31       0.00      0.00      0.00        13\n",
            "          32       0.00      0.00      0.00        10\n",
            "          33       0.00      0.00      0.00         5\n",
            "          34       1.00      0.29      0.44         7\n",
            "          35       0.00      0.00      0.00         6\n",
            "          36       0.00      0.00      0.00        11\n",
            "          37       0.00      0.00      0.00         2\n",
            "          38       0.00      0.00      0.00         3\n",
            "          39       0.00      0.00      0.00         5\n",
            "          40       0.00      0.00      0.00        10\n",
            "          41       0.00      0.00      0.00         8\n",
            "          42       0.00      0.00      0.00         3\n",
            "          43       0.00      0.00      0.00         6\n",
            "          44       0.00      0.00      0.00         5\n",
            "          45       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.71      2246\n",
            "   macro avg       0.27      0.19      0.20      2246\n",
            "weighted avg       0.68      0.71      0.67      2246\n",
            "\n",
            "정확도_랜덤포레스트_1000: 0.7782724844167409\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.58      0.61        12\n",
            "           1       0.57      0.80      0.66       105\n",
            "           2       0.73      0.40      0.52        20\n",
            "           3       0.92      0.92      0.92       813\n",
            "           4       0.72      0.91      0.80       474\n",
            "           5       0.00      0.00      0.00         5\n",
            "           6       1.00      0.50      0.67        14\n",
            "           7       1.00      0.33      0.50         3\n",
            "           8       0.75      0.71      0.73        38\n",
            "           9       0.77      0.80      0.78        25\n",
            "          10       0.88      0.93      0.90        30\n",
            "          11       0.65      0.82      0.72        83\n",
            "          12       0.71      0.38      0.50        13\n",
            "          13       0.68      0.51      0.58        37\n",
            "          14       0.00      0.00      0.00         2\n",
            "          15       0.00      0.00      0.00         9\n",
            "          16       0.66      0.76      0.70        99\n",
            "          17       0.00      0.00      0.00        12\n",
            "          18       0.73      0.55      0.63        20\n",
            "          19       0.66      0.80      0.73       133\n",
            "          20       0.79      0.43      0.56        70\n",
            "          21       0.69      0.74      0.71        27\n",
            "          22       0.00      0.00      0.00         7\n",
            "          23       0.00      0.00      0.00        12\n",
            "          24       0.50      0.11      0.17        19\n",
            "          25       0.95      0.65      0.77        31\n",
            "          26       1.00      0.12      0.22         8\n",
            "          27       1.00      0.25      0.40         4\n",
            "          28       1.00      0.10      0.18        10\n",
            "          29       0.25      0.25      0.25         4\n",
            "          30       0.83      0.42      0.56        12\n",
            "          31       0.50      0.08      0.13        13\n",
            "          32       1.00      0.10      0.18        10\n",
            "          33       1.00      0.80      0.89         5\n",
            "          34       0.75      0.43      0.55         7\n",
            "          35       1.00      0.17      0.29         6\n",
            "          36       0.67      0.36      0.47        11\n",
            "          37       1.00      0.50      0.67         2\n",
            "          38       0.00      0.00      0.00         3\n",
            "          39       0.00      0.00      0.00         5\n",
            "          40       1.00      0.30      0.46        10\n",
            "          41       0.00      0.00      0.00         8\n",
            "          42       0.00      0.00      0.00         3\n",
            "          43       0.80      0.67      0.73         6\n",
            "          44       1.00      0.80      0.89         5\n",
            "          45       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           0.78      2246\n",
            "   macro avg       0.63      0.41      0.46      2246\n",
            "weighted avg       0.77      0.78      0.76      2246\n",
            "\n",
            "정확도_그래디언트부스팅트리_1000: 0.7328584149599288\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.67      0.70        12\n",
            "           1       0.75      0.62      0.68       105\n",
            "           2       0.36      0.40      0.38        20\n",
            "           3       0.86      0.92      0.89       813\n",
            "           4       0.76      0.84      0.79       474\n",
            "           5       0.00      0.00      0.00         5\n",
            "           6       0.92      0.79      0.85        14\n",
            "           7       0.50      0.33      0.40         3\n",
            "           8       0.58      0.66      0.62        38\n",
            "           9       0.79      0.76      0.78        25\n",
            "          10       0.81      0.83      0.82        30\n",
            "          11       0.59      0.66      0.62        83\n",
            "          12       0.43      0.23      0.30        13\n",
            "          13       0.46      0.49      0.47        37\n",
            "          14       0.25      1.00      0.40         2\n",
            "          15       0.50      0.22      0.31         9\n",
            "          16       0.63      0.65      0.64        99\n",
            "          17       0.45      0.42      0.43        12\n",
            "          18       0.58      0.55      0.56        20\n",
            "          19       0.68      0.62      0.65       133\n",
            "          20       0.72      0.40      0.51        70\n",
            "          21       0.43      0.44      0.44        27\n",
            "          22       0.00      0.00      0.00         7\n",
            "          23       0.00      0.00      0.00        12\n",
            "          24       0.60      0.32      0.41        19\n",
            "          25       0.96      0.77      0.86        31\n",
            "          26       0.00      0.00      0.00         8\n",
            "          27       0.17      0.25      0.20         4\n",
            "          28       0.00      0.00      0.00        10\n",
            "          29       0.00      0.00      0.00         4\n",
            "          30       0.50      0.25      0.33        12\n",
            "          31       0.40      0.15      0.22        13\n",
            "          32       1.00      0.50      0.67        10\n",
            "          33       0.50      0.60      0.55         5\n",
            "          34       0.67      0.29      0.40         7\n",
            "          35       1.00      0.33      0.50         6\n",
            "          36       0.23      0.27      0.25        11\n",
            "          37       0.00      0.00      0.00         2\n",
            "          38       0.00      0.00      0.00         3\n",
            "          39       0.25      0.20      0.22         5\n",
            "          40       0.29      0.20      0.24        10\n",
            "          41       0.00      0.00      0.00         8\n",
            "          42       0.00      0.00      0.00         3\n",
            "          43       0.29      0.33      0.31         6\n",
            "          44       1.00      0.60      0.75         5\n",
            "          45       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.73      2246\n",
            "   macro avg       0.45      0.38      0.39      2246\n",
            "weighted avg       0.73      0.73      0.72      2246\n",
            "\n",
            "정확도_나이브_3000: 0.691006233303651\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.25      0.40        12\n",
            "           1       0.48      0.83      0.61       105\n",
            "           2       0.00      0.00      0.00        20\n",
            "           3       0.88      0.88      0.88       813\n",
            "           4       0.66      0.94      0.77       474\n",
            "           5       0.00      0.00      0.00         5\n",
            "           6       0.00      0.00      0.00        14\n",
            "           7       0.00      0.00      0.00         3\n",
            "           8       0.00      0.00      0.00        38\n",
            "           9       1.00      0.48      0.65        25\n",
            "          10       1.00      0.17      0.29        30\n",
            "          11       0.44      0.77      0.56        83\n",
            "          12       0.00      0.00      0.00        13\n",
            "          13       0.75      0.24      0.37        37\n",
            "          14       0.00      0.00      0.00         2\n",
            "          15       0.00      0.00      0.00         9\n",
            "          16       0.55      0.79      0.64        99\n",
            "          17       0.00      0.00      0.00        12\n",
            "          18       0.67      0.10      0.17        20\n",
            "          19       0.48      0.82      0.61       133\n",
            "          20       0.93      0.20      0.33        70\n",
            "          21       1.00      0.22      0.36        27\n",
            "          22       0.00      0.00      0.00         7\n",
            "          23       0.00      0.00      0.00        12\n",
            "          24       0.00      0.00      0.00        19\n",
            "          25       1.00      0.13      0.23        31\n",
            "          26       0.00      0.00      0.00         8\n",
            "          27       0.00      0.00      0.00         4\n",
            "          28       0.00      0.00      0.00        10\n",
            "          29       0.00      0.00      0.00         4\n",
            "          30       0.00      0.00      0.00        12\n",
            "          31       0.00      0.00      0.00        13\n",
            "          32       0.00      0.00      0.00        10\n",
            "          33       0.00      0.00      0.00         5\n",
            "          34       0.00      0.00      0.00         7\n",
            "          35       0.00      0.00      0.00         6\n",
            "          36       0.00      0.00      0.00        11\n",
            "          37       0.00      0.00      0.00         2\n",
            "          38       0.00      0.00      0.00         3\n",
            "          39       0.00      0.00      0.00         5\n",
            "          40       0.00      0.00      0.00        10\n",
            "          41       0.00      0.00      0.00         8\n",
            "          42       0.00      0.00      0.00         3\n",
            "          43       0.00      0.00      0.00         6\n",
            "          44       0.00      0.00      0.00         5\n",
            "          45       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.69      2246\n",
            "   macro avg       0.24      0.15      0.15      2246\n",
            "weighted avg       0.65      0.69      0.63      2246\n",
            "\n",
            "정확도_랜덤포레스트_3000: 0.7747105966162066\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.58      0.70        12\n",
            "           1       0.56      0.83      0.67       105\n",
            "           2       0.86      0.60      0.71        20\n",
            "           3       0.93      0.91      0.92       813\n",
            "           4       0.71      0.90      0.80       474\n",
            "           5       0.00      0.00      0.00         5\n",
            "           6       1.00      0.64      0.78        14\n",
            "           7       1.00      0.33      0.50         3\n",
            "           8       0.72      0.68      0.70        38\n",
            "           9       0.86      0.72      0.78        25\n",
            "          10       0.89      0.83      0.86        30\n",
            "          11       0.66      0.81      0.72        83\n",
            "          12       0.50      0.15      0.24        13\n",
            "          13       0.61      0.51      0.56        37\n",
            "          14       0.00      0.00      0.00         2\n",
            "          15       0.00      0.00      0.00         9\n",
            "          16       0.66      0.75      0.70        99\n",
            "          17       0.00      0.00      0.00        12\n",
            "          18       0.58      0.55      0.56        20\n",
            "          19       0.64      0.78      0.71       133\n",
            "          20       0.71      0.43      0.54        70\n",
            "          21       0.83      0.74      0.78        27\n",
            "          22       0.00      0.00      0.00         7\n",
            "          23       0.25      0.08      0.12        12\n",
            "          24       0.60      0.16      0.25        19\n",
            "          25       0.95      0.58      0.72        31\n",
            "          26       1.00      0.38      0.55         8\n",
            "          27       1.00      0.25      0.40         4\n",
            "          28       1.00      0.10      0.18        10\n",
            "          29       0.50      0.75      0.60         4\n",
            "          30       0.50      0.08      0.14        12\n",
            "          31       0.50      0.08      0.13        13\n",
            "          32       1.00      0.30      0.46        10\n",
            "          33       1.00      0.80      0.89         5\n",
            "          34       0.75      0.43      0.55         7\n",
            "          35       1.00      0.17      0.29         6\n",
            "          36       0.60      0.27      0.37        11\n",
            "          37       1.00      0.50      0.67         2\n",
            "          38       0.00      0.00      0.00         3\n",
            "          39       0.00      0.00      0.00         5\n",
            "          40       1.00      0.30      0.46        10\n",
            "          41       0.50      0.12      0.20         8\n",
            "          42       0.00      0.00      0.00         3\n",
            "          43       0.75      0.50      0.60         6\n",
            "          44       1.00      0.80      0.89         5\n",
            "          45       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           0.77      2246\n",
            "   macro avg       0.64      0.42      0.47      2246\n",
            "weighted avg       0.77      0.77      0.76      2246\n",
            "\n",
            "정확도_그래디언트부스팅트리_3000: 0.7693677649154052\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.58      0.70        12\n",
            "           1       0.77      0.69      0.73       105\n",
            "           2       0.67      0.70      0.68        20\n",
            "           3       0.89      0.91      0.90       813\n",
            "           4       0.75      0.84      0.79       474\n",
            "           5       0.00      0.00      0.00         5\n",
            "           6       0.92      0.86      0.89        14\n",
            "           7       0.33      0.33      0.33         3\n",
            "           8       0.62      0.66      0.64        38\n",
            "           9       0.91      0.80      0.85        25\n",
            "          10       0.83      0.83      0.83        30\n",
            "          11       0.62      0.65      0.64        83\n",
            "          12       0.33      0.23      0.27        13\n",
            "          13       0.53      0.43      0.48        37\n",
            "          14       0.25      0.50      0.33         2\n",
            "          15       0.43      0.33      0.38         9\n",
            "          16       0.73      0.75      0.74        99\n",
            "          17       0.50      0.50      0.50        12\n",
            "          18       0.71      0.50      0.59        20\n",
            "          19       0.72      0.67      0.70       133\n",
            "          20       0.56      0.44      0.50        70\n",
            "          21       0.53      0.67      0.59        27\n",
            "          22       0.33      0.14      0.20         7\n",
            "          23       0.50      0.67      0.57        12\n",
            "          24       0.56      0.53      0.54        19\n",
            "          25       0.81      0.68      0.74        31\n",
            "          26       0.75      0.75      0.75         8\n",
            "          27       0.25      0.25      0.25         4\n",
            "          28       0.43      0.30      0.35        10\n",
            "          29       0.14      0.25      0.18         4\n",
            "          30       0.50      0.50      0.50        12\n",
            "          31       0.50      0.23      0.32        13\n",
            "          32       1.00      0.70      0.82        10\n",
            "          33       1.00      0.80      0.89         5\n",
            "          34       0.40      0.29      0.33         7\n",
            "          35       0.80      0.67      0.73         6\n",
            "          36       0.67      0.55      0.60        11\n",
            "          37       1.00      1.00      1.00         2\n",
            "          38       0.50      0.33      0.40         3\n",
            "          39       0.20      0.20      0.20         5\n",
            "          40       0.60      0.30      0.40        10\n",
            "          41       0.56      0.62      0.59         8\n",
            "          42       1.00      0.67      0.80         3\n",
            "          43       0.60      0.50      0.55         6\n",
            "          44       1.00      0.80      0.89         5\n",
            "          45       0.50      1.00      0.67         1\n",
            "\n",
            "    accuracy                           0.77      2246\n",
            "   macro avg       0.61      0.56      0.57      2246\n",
            "weighted avg       0.77      0.77      0.76      2246\n",
            "\n",
            "정확도_나이브_5000: 0.6780943900267141\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        12\n",
            "           1       0.50      0.81      0.62       105\n",
            "           2       0.00      0.00      0.00        20\n",
            "           3       0.85      0.89      0.87       813\n",
            "           4       0.61      0.95      0.74       474\n",
            "           5       0.00      0.00      0.00         5\n",
            "           6       0.00      0.00      0.00        14\n",
            "           7       0.00      0.00      0.00         3\n",
            "           8       0.00      0.00      0.00        38\n",
            "           9       1.00      0.32      0.48        25\n",
            "          10       0.00      0.00      0.00        30\n",
            "          11       0.48      0.75      0.58        83\n",
            "          12       0.00      0.00      0.00        13\n",
            "          13       1.00      0.16      0.28        37\n",
            "          14       0.00      0.00      0.00         2\n",
            "          15       0.00      0.00      0.00         9\n",
            "          16       0.61      0.73      0.66        99\n",
            "          17       0.00      0.00      0.00        12\n",
            "          18       0.00      0.00      0.00        20\n",
            "          19       0.49      0.81      0.61       133\n",
            "          20       0.90      0.13      0.23        70\n",
            "          21       1.00      0.04      0.07        27\n",
            "          22       0.00      0.00      0.00         7\n",
            "          23       0.00      0.00      0.00        12\n",
            "          24       0.00      0.00      0.00        19\n",
            "          25       1.00      0.10      0.18        31\n",
            "          26       0.00      0.00      0.00         8\n",
            "          27       0.00      0.00      0.00         4\n",
            "          28       0.00      0.00      0.00        10\n",
            "          29       0.00      0.00      0.00         4\n",
            "          30       0.00      0.00      0.00        12\n",
            "          31       0.00      0.00      0.00        13\n",
            "          32       0.00      0.00      0.00        10\n",
            "          33       0.00      0.00      0.00         5\n",
            "          34       0.00      0.00      0.00         7\n",
            "          35       0.00      0.00      0.00         6\n",
            "          36       0.00      0.00      0.00        11\n",
            "          37       0.00      0.00      0.00         2\n",
            "          38       0.00      0.00      0.00         3\n",
            "          39       0.00      0.00      0.00         5\n",
            "          40       0.00      0.00      0.00        10\n",
            "          41       0.00      0.00      0.00         8\n",
            "          42       0.00      0.00      0.00         3\n",
            "          43       0.00      0.00      0.00         6\n",
            "          44       0.00      0.00      0.00         5\n",
            "          45       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.68      2246\n",
            "   macro avg       0.18      0.12      0.12      2246\n",
            "weighted avg       0.62      0.68      0.61      2246\n",
            "\n",
            "정확도_랜덤포레스트_5000: 0.7658058771148709\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.50      0.63        12\n",
            "           1       0.59      0.80      0.68       105\n",
            "           2       0.83      0.50      0.62        20\n",
            "           3       0.91      0.91      0.91       813\n",
            "           4       0.70      0.91      0.79       474\n",
            "           5       0.00      0.00      0.00         5\n",
            "           6       1.00      0.57      0.73        14\n",
            "           7       1.00      0.33      0.50         3\n",
            "           8       0.72      0.68      0.70        38\n",
            "           9       0.87      0.80      0.83        25\n",
            "          10       0.86      0.80      0.83        30\n",
            "          11       0.61      0.83      0.70        83\n",
            "          12       0.60      0.23      0.33        13\n",
            "          13       0.62      0.54      0.58        37\n",
            "          14       0.00      0.00      0.00         2\n",
            "          15       0.00      0.00      0.00         9\n",
            "          16       0.69      0.71      0.70        99\n",
            "          17       0.00      0.00      0.00        12\n",
            "          18       0.52      0.55      0.54        20\n",
            "          19       0.66      0.77      0.71       133\n",
            "          20       0.71      0.43      0.54        70\n",
            "          21       0.76      0.59      0.67        27\n",
            "          22       0.00      0.00      0.00         7\n",
            "          23       0.25      0.08      0.12        12\n",
            "          24       0.50      0.11      0.17        19\n",
            "          25       0.94      0.48      0.64        31\n",
            "          26       1.00      0.25      0.40         8\n",
            "          27       1.00      0.25      0.40         4\n",
            "          28       0.50      0.10      0.17        10\n",
            "          29       0.25      0.25      0.25         4\n",
            "          30       1.00      0.17      0.29        12\n",
            "          31       0.50      0.08      0.13        13\n",
            "          32       1.00      0.40      0.57        10\n",
            "          33       1.00      0.80      0.89         5\n",
            "          34       0.60      0.43      0.50         7\n",
            "          35       1.00      0.17      0.29         6\n",
            "          36       0.50      0.18      0.27        11\n",
            "          37       0.00      0.00      0.00         2\n",
            "          38       0.00      0.00      0.00         3\n",
            "          39       0.00      0.00      0.00         5\n",
            "          40       1.00      0.20      0.33        10\n",
            "          41       0.50      0.12      0.20         8\n",
            "          42       0.00      0.00      0.00         3\n",
            "          43       0.75      0.50      0.60         6\n",
            "          44       1.00      0.80      0.89         5\n",
            "          45       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           0.77      2246\n",
            "   macro avg       0.60      0.39      0.44      2246\n",
            "weighted avg       0.76      0.77      0.74      2246\n",
            "\n",
            "정확도_그래디언트부스팅트리_5000: 0.0013357079252003562\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        12\n",
            "           1       0.00      0.00      0.00       105\n",
            "           2       0.00      0.00      0.00        20\n",
            "           3       0.00      0.00      0.00       813\n",
            "           4       0.00      0.00      0.00       474\n",
            "           5       0.00      0.00      0.00         5\n",
            "           6       0.00      0.00      0.00        14\n",
            "           7       0.00      0.00      0.00         3\n",
            "           8       0.00      0.00      0.00        38\n",
            "           9       0.00      0.00      0.00        25\n",
            "          10       0.00      0.00      0.00        30\n",
            "          11       0.00      0.00      0.00        83\n",
            "          12       0.00      0.00      0.00        13\n",
            "          13       0.00      0.00      0.00        37\n",
            "          14       0.00      0.00      0.00         2\n",
            "          15       0.00      0.00      0.00         9\n",
            "          16       0.00      0.00      0.00        99\n",
            "          17       0.00      0.00      0.00        12\n",
            "          18       0.00      0.00      0.00        20\n",
            "          19       0.00      0.00      0.00       133\n",
            "          20       0.00      0.00      0.00        70\n",
            "          21       0.00      0.00      0.00        27\n",
            "          22       0.00      0.00      0.00         7\n",
            "          23       0.00      0.25      0.00        12\n",
            "          24       0.00      0.00      0.00        19\n",
            "          25       0.00      0.00      0.00        31\n",
            "          26       0.00      0.00      0.00         8\n",
            "          27       0.00      0.00      0.00         4\n",
            "          28       0.00      0.00      0.00        10\n",
            "          29       0.00      0.00      0.00         4\n",
            "          30       0.00      0.00      0.00        12\n",
            "          31       0.00      0.00      0.00        13\n",
            "          32       0.00      0.00      0.00        10\n",
            "          33       0.00      0.00      0.00         5\n",
            "          34       0.00      0.00      0.00         7\n",
            "          35       0.00      0.00      0.00         6\n",
            "          36       0.00      0.00      0.00        11\n",
            "          37       0.00      0.00      0.00         2\n",
            "          38       0.00      0.00      0.00         3\n",
            "          39       0.00      0.00      0.00         5\n",
            "          40       0.00      0.00      0.00        10\n",
            "          41       0.00      0.00      0.00         8\n",
            "          42       0.00      0.00      0.00         3\n",
            "          43       0.00      0.00      0.00         6\n",
            "          44       0.00      0.00      0.00         5\n",
            "          45       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.00      2246\n",
            "   macro avg       0.00      0.01      0.00      2246\n",
            "weighted avg       0.00      0.00      0.00      2246\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "어? 와이라누???? 그래디언트 부스팅 트리 5000 파라미터 변경해보기"
      ],
      "metadata": {
        "id": "4w56mmBnyTXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=5000, test_split=0.2)\n",
        "decoded_train = []\n",
        "decoded_test = []\n",
        "\n",
        "# 훈련 데이터 디코딩\n",
        "for seq in x_train:\n",
        "    decoded_seq = ' '.join([reverse_word_index.get(index - 3, '?') for index in seq])\n",
        "    if decoded_seq.strip():  # 빈 문장은 제외\n",
        "        decoded_train.append(decoded_seq)\n",
        "\n",
        "# 테스트 데이터 디코딩\n",
        "for seq in x_test:\n",
        "    decoded_seq = ' '.join([reverse_word_index.get(index - 3, '?') for index in seq])\n",
        "    if decoded_seq.strip():  # 빈 문장은 제외\n",
        "        decoded_test.append(decoded_seq)\n",
        "\n",
        "x_train = decoded_train\n",
        "x_test = decoded_test\n",
        "\n",
        "# DTM 생성하기\n",
        "dtmvector = CountVectorizer(stop_words=None)\n",
        "x_train_dtm = dtmvector.fit_transform(x_train)\n",
        "x_test_dtm = dtmvector.transform(x_test)\n",
        "\n",
        "# TF-IDF 생성하기\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "x_train_tfidf = tfidf_transformer.fit_transform(x_train_dtm)\n",
        "x_test_tfidf = tfidf_transformer.transform(x_test_dtm)\n",
        "\n",
        "# 그래디언트 부스팅 트리\n",
        "gbc = GradientBoostingClassifier(random_state=0, max_depth =1).fit(x_train_tfidf, y_train)#원래 기본값 3인데 1로 줄임 (과적합 방지)\n",
        "predicted = gbc.predict(x_test_tfidf)\n",
        "print(f\"정확도_그래디언트부스팅트리_{i}: {accuracy_score(y_test, predicted)}\")\n",
        "print(classification_report(y_test, predicted, zero_division=0))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILMY5pH6zL4p",
        "outputId": "14c99235-d675-4ae7-bdf9-fdcf73f9d994"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정확도_그래디언트부스팅트리_5000: 0.784060552092609\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.67      0.76        12\n",
            "           1       0.85      0.70      0.76       105\n",
            "           2       0.67      0.70      0.68        20\n",
            "           3       0.93      0.91      0.92       813\n",
            "           4       0.70      0.88      0.78       474\n",
            "           5       0.00      0.00      0.00         5\n",
            "           6       0.93      0.93      0.93        14\n",
            "           7       1.00      0.33      0.50         3\n",
            "           8       0.66      0.71      0.68        38\n",
            "           9       0.92      0.88      0.90        25\n",
            "          10       0.84      0.87      0.85        30\n",
            "          11       0.66      0.67      0.67        83\n",
            "          12       1.00      0.46      0.63        13\n",
            "          13       0.61      0.51      0.56        37\n",
            "          14       0.00      0.00      0.00         2\n",
            "          15       0.31      0.56      0.40         9\n",
            "          16       0.72      0.78      0.75        99\n",
            "          17       0.38      0.25      0.30        12\n",
            "          18       0.48      0.55      0.51        20\n",
            "          19       0.73      0.63      0.68       133\n",
            "          20       0.64      0.49      0.55        70\n",
            "          21       0.59      0.48      0.53        27\n",
            "          22       0.40      0.29      0.33         7\n",
            "          23       0.60      0.75      0.67        12\n",
            "          24       0.79      0.58      0.67        19\n",
            "          25       0.87      0.87      0.87        31\n",
            "          26       0.83      0.62      0.71         8\n",
            "          27       0.67      0.50      0.57         4\n",
            "          28       0.50      0.20      0.29        10\n",
            "          29       0.33      0.25      0.29         4\n",
            "          30       0.00      0.00      0.00        12\n",
            "          31       0.90      0.69      0.78        13\n",
            "          32       0.90      0.90      0.90        10\n",
            "          33       0.67      0.40      0.50         5\n",
            "          34       0.86      0.86      0.86         7\n",
            "          35       1.00      0.67      0.80         6\n",
            "          36       0.54      0.64      0.58        11\n",
            "          37       0.50      1.00      0.67         2\n",
            "          38       0.00      0.00      0.00         3\n",
            "          39       0.00      0.00      0.00         5\n",
            "          40       1.00      0.10      0.18        10\n",
            "          41       1.00      0.12      0.22         8\n",
            "          42       0.00      0.00      0.00         3\n",
            "          43       0.43      0.50      0.46         6\n",
            "          44       1.00      0.80      0.89         5\n",
            "          45       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.78      2246\n",
            "   macro avg       0.61      0.52      0.53      2246\n",
            "weighted avg       0.78      0.78      0.78      2246\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> 0.78 나온 그래디언트 부스팅 5000 선택. 아마 이전거는..... 심각하게 과적합된 게 아닐까?"
      ],
      "metadata": {
        "id": "w3PEY-QA-Qw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalMaxPooling1D, Dropout, Conv1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "BHMpoKGWkjXd"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5000\n",
        "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=vocab_size, test_split=0.2)"
      ],
      "metadata": {
        "id": "dZI5F9gvCwc1"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#훈련용 데이터셋 길이 분포\n",
        "train_lengths = [len(seq) for seq in X_train]\n",
        "percentiles = np.percentile(train_lengths, [50, 75, 90, 95, 99])\n",
        "mean_length = np.mean(train_lengths)\n",
        "std_length = np.std(train_lengths)\n",
        "mean_2sd_range = (mean_length - 2 * std_length, mean_length + 2 * std_length)\n",
        "\n",
        "print(f\"\\nnum_words={i}에 대한 통계 정보:\")\n",
        "print(f\"50% 길이: {percentiles[0]}\")\n",
        "print(f\"75% 길이: {percentiles[1]}\")\n",
        "print(f\"90% 길이: {percentiles[2]}\")\n",
        "print(f\"95% 길이: {percentiles[3]}\")\n",
        "print(f\"99% 길이: {percentiles[4]}\")\n",
        "print(f\"평균 길이: {mean_length}\")\n",
        "print(f\"2SD 범위: {mean_2sd_range}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ptm6F_U-DGR1",
        "outputId": "9c75ea7a-0bd2-4916-e72b-b3ae8f597d91"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "num_words=5000에 대한 통계 정보:\n",
            "50% 길이: 95.0\n",
            "75% 길이: 179.0\n",
            "90% 길이: 313.0\n",
            "95% 길이: 459.0\n",
            "99% 길이: 718.0\n",
            "평균 길이: 145.5398574927633\n",
            "2SD 범위: (-146.08840277542225, 437.16811776094886)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 300\n",
        "X_train_pad = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test_pad = pad_sequences(X_test, maxlen=max_len)"
      ],
      "metadata": {
        "id": "UA4cbzMgGyqP"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('X_train의 크기(shape) :',X_train_pad.shape)\n",
        "print('X_test의 크기(shape) :',X_test_pad.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hH0oSwIzG3NU",
        "outputId": "0d6bb1bf-d0a2-496e-9774-27516e8adf51"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train의 크기(shape) : (8982, 300)\n",
            "X_test의 크기(shape) : (2246, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 256\n",
        "dropout_ratio = 0.2\n",
        "num_filters = 256  #커널 수\n",
        "kernel_size = 3\n",
        "hidden_units = 128 #뉴런 수"
      ],
      "metadata": {
        "id": "OohaeSrRyXsn"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(input_dim=5000, output_dim=embedding_dim), #vocab_size, embedding dim\n",
        "    Dropout(dropout_ratio),\n",
        "    Conv1D(num_filters, kernel_size, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(hidden_units, activation='relu'),\n",
        "    Dropout(dropout_ratio),\n",
        "    Dense(46, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "lZvYKrY6-i2_"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yi2HYcFWzaD-",
        "outputId": "dc6c6c2b-970d-4b23-f862-223ed9ab8621"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, None, 256)         1280000   \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, None, 256)         0         \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, None, 256)         196864    \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Gl  (None, 256)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 46)                5934      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1515694 (5.78 MB)\n",
            "Trainable params: 1515694 (5.78 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
        "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True)"
      ],
      "metadata": {
        "id": "rG_dm97A_oAW"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(X_train_pad, y_train, epochs=10, batch_size=64, validation_data =(X_test_pad, y_test), callbacks=[es, model_checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7xo32OnABfM",
        "outputId": "0a736bc9-23ae-4c5e-d0f8-1f6b01d57eb8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "141/141 [==============================] - 83s 574ms/step - loss: 1.9423 - accuracy: 0.5370 - val_loss: 1.3799 - val_accuracy: 0.6736\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "141/141 [==============================] - 89s 633ms/step - loss: 1.1659 - accuracy: 0.7280 - val_loss: 1.0426 - val_accuracy: 0.7538\n",
            "Epoch 3/10\n",
            "141/141 [==============================] - 76s 541ms/step - loss: 0.8696 - accuracy: 0.7937 - val_loss: 0.9004 - val_accuracy: 0.7854\n",
            "Epoch 4/10\n",
            "141/141 [==============================] - 81s 571ms/step - loss: 0.6721 - accuracy: 0.8342 - val_loss: 0.8523 - val_accuracy: 0.7983\n",
            "Epoch 5/10\n",
            "141/141 [==============================] - 77s 547ms/step - loss: 0.5165 - accuracy: 0.8713 - val_loss: 0.8514 - val_accuracy: 0.7956\n",
            "Epoch 6/10\n",
            "141/141 [==============================] - 77s 543ms/step - loss: 0.4019 - accuracy: 0.8986 - val_loss: 0.8360 - val_accuracy: 0.8054\n",
            "Epoch 7/10\n",
            "141/141 [==============================] - 76s 543ms/step - loss: 0.3076 - accuracy: 0.9196 - val_loss: 0.8932 - val_accuracy: 0.8063\n",
            "Epoch 8/10\n",
            "141/141 [==============================] - 79s 558ms/step - loss: 0.2450 - accuracy: 0.9352 - val_loss: 0.9312 - val_accuracy: 0.8059\n",
            "Epoch 9/10\n",
            "141/141 [==============================] - 76s 536ms/step - loss: 0.2217 - accuracy: 0.9400 - val_loss: 0.9509 - val_accuracy: 0.8063\n",
            "Epoch 9: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = load_model('best_model.h5')\n",
        "print(\"테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test_pad, y_test)[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mwE_vSsAYSn",
        "outputId": "e304adf4-abbb-496b-e968-10e533e5ab19"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71/71 [==============================] - 5s 66ms/step - loss: 0.8360 - accuracy: 0.8054\n",
            "테스트 정확도: 0.8054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **회고-1**\n",
        "\n",
        "- 딥러닝 1D CNN 이 가장 성능이 우수했다.\n",
        "- ML 모델들 중에서는 그래디언트 부스팅 (중간에 이상하긴 했지만) 이 가장 성능이 좋았다.\n",
        "- 오늘 하면서 알게 된 건데 1d cnn에서 average poolinng보다 maxpooling을 사용하는 것이 이 테스크의 목적에 부합한다. 여러 가지 특징들 중 가장 중요한 것만뽑아주는 max pooling은, 특징을 다소 희석하는 average pooling보다 텍스트 분류 작업에서 좋을 수 있다. 보통 자연어 처리같은 데이터에서 맥스 풀링이 더 나은 성능을 보인다고 한다.\n",
        "- 근데 1D CNN은 accuracy는 높지만 loss도 높다. 이건 아마... 데이터셋에 클래스 불균형이 있어서 그런 걸수도 있다. 가령 이 데이터는 class 3,4만 엄청 많다. 이럴 때는 특정 클래스에 과대적합, 과소적합 될 수 있다. 그리고 손실 함수는 모든 샘플에 대해 동일한 가중치를 부여하는데, 이 때 지배적인 클래스의 손실이 전체 손실 값이 큰 영향을 미치게 된다."
      ],
      "metadata": {
        "id": "7UAnI1BDLSWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# 클래스 라벨\n",
        "classes = np.unique(y_train)\n",
        "# 클래스별 샘플 수\n",
        "class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
        "# 클래스 가중치를 딕셔너리로 변환\n",
        "class_weight_dict = dict(zip(classes, class_weights))"
      ],
      "metadata": {
        "id": "bO2xVp17LRuJ"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(input_dim=5000, output_dim=embedding_dim), #vocab_size, embedding dim\n",
        "    Dropout(dropout_ratio),\n",
        "    Conv1D(num_filters, kernel_size, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(hidden_units, activation='relu'),\n",
        "    Dropout(dropout_ratio),\n",
        "    Dense(46, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "b-EiOxAzOqxU"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(X_train_pad, y_train, epochs=10, batch_size=64, validation_data =(X_test_pad, y_test), class_weight = class_weight_dict, callbacks=[es, model_checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z61DzBP4K7Ao",
        "outputId": "ee27dcb1-5299-4a83-d9a8-6f854bdff809"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "141/141 [==============================] - 81s 554ms/step - loss: 3.8007 - accuracy: 0.0953 - val_loss: 3.5725 - val_accuracy: 0.3615\n",
            "Epoch 2/10\n",
            "141/141 [==============================] - 92s 651ms/step - loss: 3.0834 - accuracy: 0.4292 - val_loss: 2.0435 - val_accuracy: 0.6300\n",
            "Epoch 3/10\n",
            "141/141 [==============================] - 80s 568ms/step - loss: 1.6584 - accuracy: 0.6348 - val_loss: 1.7045 - val_accuracy: 0.6705\n",
            "Epoch 4/10\n",
            "141/141 [==============================] - 85s 604ms/step - loss: 1.0427 - accuracy: 0.6973 - val_loss: 1.2763 - val_accuracy: 0.7115\n",
            "Epoch 5/10\n",
            "141/141 [==============================] - 77s 550ms/step - loss: 0.7133 - accuracy: 0.7308 - val_loss: 1.4101 - val_accuracy: 0.6821\n",
            "Epoch 6/10\n",
            "141/141 [==============================] - 77s 547ms/step - loss: 0.5149 - accuracy: 0.7690 - val_loss: 1.1228 - val_accuracy: 0.7449\n",
            "Epoch 7/10\n",
            "141/141 [==============================] - 86s 613ms/step - loss: 0.3975 - accuracy: 0.8073 - val_loss: 1.0373 - val_accuracy: 0.7605\n",
            "Epoch 8/10\n",
            "141/141 [==============================] - 83s 589ms/step - loss: 0.3090 - accuracy: 0.8338 - val_loss: 0.9805 - val_accuracy: 0.7792\n",
            "Epoch 9/10\n",
            "141/141 [==============================] - 77s 550ms/step - loss: 0.2701 - accuracy: 0.8445 - val_loss: 1.0495 - val_accuracy: 0.7689\n",
            "Epoch 10/10\n",
            "141/141 [==============================] - 80s 566ms/step - loss: 0.2248 - accuracy: 0.8619 - val_loss: 0.9830 - val_accuracy: 0.7872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**회고-2**\n",
        "아까보다 성능이 더 안좋다. 암만 가중치를 줬다고 해도 데이터 수 자체가 적어서 그런 걸까?"
      ],
      "metadata": {
        "id": "10C8PzFgS1v9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "class_counts = dict(zip(unique, counts))\n",
        "print(class_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlP9JUrKSAeo",
        "outputId": "ec226713-f112-40c5-c443-ea7a9b3046f9"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 55, 1: 432, 2: 74, 3: 3159, 4: 1949, 5: 17, 6: 48, 7: 16, 8: 139, 9: 101, 10: 124, 11: 390, 12: 49, 13: 172, 14: 26, 15: 20, 16: 444, 17: 39, 18: 66, 19: 549, 20: 269, 21: 100, 22: 15, 23: 41, 24: 62, 25: 92, 26: 24, 27: 15, 28: 48, 29: 19, 30: 45, 31: 39, 32: 32, 33: 11, 34: 50, 35: 10, 36: 49, 37: 19, 38: 19, 39: 24, 40: 36, 41: 30, 42: 13, 43: 21, 44: 12, 45: 18}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique, counts = np.unique(y_test, return_counts=True)\n",
        "class_counts = dict(zip(unique, counts))\n",
        "print(class_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWmIR5hWS89Y",
        "outputId": "d26f89df-602c-4bc4-848c-edb3c2a5ab19"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 12, 1: 105, 2: 20, 3: 813, 4: 474, 5: 5, 6: 14, 7: 3, 8: 38, 9: 25, 10: 30, 11: 83, 12: 13, 13: 37, 14: 2, 15: 9, 16: 99, 17: 12, 18: 20, 19: 133, 20: 70, 21: 27, 22: 7, 23: 12, 24: 19, 25: 31, 26: 8, 27: 4, 28: 10, 29: 4, 30: 12, 31: 13, 32: 10, 33: 5, 34: 7, 35: 6, 36: 11, 37: 2, 38: 3, 39: 5, 40: 10, 41: 8, 42: 3, 43: 6, 44: 5, 45: 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DUYm7ViQTJaq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
